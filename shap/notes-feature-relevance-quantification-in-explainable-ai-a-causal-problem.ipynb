{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper: https://arxiv.org/abs/1910.13413"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature relevance quantification in explainable ai: A causal problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More discussion for Eq. (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{E}[Y|x_1, x_2, x_3] \n",
    "&= \\int y p(y|x_1, x_2, x_3) dy \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{E}[Y|X_1 = x_1] \n",
    "&= \\int y p(y|x_1) dy \\\\\n",
    "&= \\int y \\left(  \\iint p(y, x_2, x_3 | x_1) dx_2 dx_3 \\right ) dy \\\\\n",
    "&= \\int y \\left(  \\iint p(y | x_1, x_2, x_3) p(x_2, x_3 | x_1) dx_2 dx_3 \\right ) dy \\\\\n",
    "&= \\iiint y p(y|x_1, x_2, x_3) p(x_2, x_3 | x_1) dy dx_2 dx_3 \\\\\n",
    "&= \\iint\\mathbb{E}[Y|x_1, x_2, x_3] p(x_2, x_3 | x_1) dx_2 dx_3\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{E}[Y|do(X_1=x_1)] \n",
    "&= \\iint\\mathbb{E}[Y|x_1, x_2, x_3] p(x_2, x_3) dx_2 dx_3\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes from **[Causal Interpretations of Black-Box Models](https://www.tandfonline.com/doi/full/10.1080/07350015.2019.1624293)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are at least three notions of feature importance:\n",
    "\n",
    "* How big impact does each feature contributes to the prediction. E.g. the coefficients of a linear regression model. SHAP also belongs to this category. Note, the notion of feature importance here has nothing to do with the performance of the a model.\n",
    "* The importance of a feature in contribution to the predictive accuracy. E.g. for decision trees, some form of split gain summary for a feature belongs to this category.\n",
    "* Causality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
