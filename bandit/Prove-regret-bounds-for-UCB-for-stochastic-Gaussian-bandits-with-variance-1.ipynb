{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e4e789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b23686",
   "metadata": {},
   "source": [
    "The proof of the bound is adopted from the Youtube lecture [Bandit Algorithm - 1](https://www.youtube.com/watch?v=xN11-epRuSU&list=PLf6zO2kXw8HqVxnPXF7GF_otdhJg_vIl-) by Tor Lattimore. It goes in three steps:\n",
    "\n",
    "* Concentration analysis for Gaussian r.v. with known variance of 1, which bounds the mean of each arm.\n",
    "* Decomposition of regret into a product of suboptimality gap $\\Delta_a$ and number of plays for each action $T_a(n)$.\n",
    "* Bound $T_a(t)$ as a function $\\Delta_a$ using the UCB trick. Setting $\\delta=\\frac{1}{n^2}$. Then, consider actions separately for $\\Delta_a$ above or below a cutoff $\\Delta$, setting $\\Delta=\\sqrt{\\frac{k \\log n}{n}}$, we obtain $R_n = O(\\sqrt{nk\\log n})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe5c29",
   "metadata": {},
   "source": [
    "# Concentration analysis for mean of $\\mathcal{N}(\\mu, 1)$ r.v.s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e28392",
   "metadata": {},
   "source": [
    "The output of concentration analysis is in the form of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9f47d",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{P}(\\hat{\\mu} \\ge \\mu + \\epsilon(\\delta)) &\\le \\delta \\\\\n",
    "\\mathbb{P}(\\hat{\\mu} \\le \\mu - \\epsilon(\\delta)) &\\le \\delta \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17970160",
   "metadata": {},
   "source": [
    "which leads to\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(|\\hat{\\mu} - \\mu| \\le \\epsilon(\\delta)) \\ge 1 - 2 \\delta\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc4d27d",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "* $\\hat{\\mu}$ is the esitmated mean of a random variable.\n",
    "* $\\hat{\\mu}$ is the expectation of the random vaiable.\n",
    "* $\\delta \\in (0, 1)$, and is called the confidence level.\n",
    "* $\\epsilon(\\delta) > 0$ is a cutoff, $[\\mu - \\epsilon(\\delta), \\mu + \\epsilon(\\delta)]$ is called the confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a63a8",
   "metadata": {},
   "source": [
    "For a sequence of $T$ IID Gaussian random variables with mean $\\mu$ and variance $1$, $X_1, \\cdots, X_T$, let"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2983afd",
   "metadata": {},
   "source": [
    "$$\\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^T X_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada7469",
   "metadata": {},
   "source": [
    "We can show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40753c32",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathbb{P}(\\hat{\\mu} \\ge \\mu + \\epsilon) \n",
    "&\\le \\exp \\left( - \\frac{\\epsilon^2T}{2} \\right) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38588a22",
   "metadata": {},
   "source": [
    "for a positive $\\epsilon$. See Appendix for a proof."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fecec4d",
   "metadata": {},
   "source": [
    "Let \n",
    "\n",
    "\\begin{align*}\n",
    "\\delta &= \\exp \\left(- \\frac{\\epsilon^2 T}{2} \\right) \\\\\n",
    "\\epsilon &= \\sqrt{\\frac{2 \\log (1 / \\delta)}{T}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aecf32",
   "metadata": {},
   "source": [
    "Then, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737433ce",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{P} \\left(\\hat{\\mu} \\ge \\mu + \\sqrt{\\frac{2 \\log(1 / \\delta)}{T}} \\right) \n",
    "&\\le \\delta \\\\\n",
    "\\mathbb{P} \\left(\\hat{\\mu} \\le \\mu - \\sqrt{\\frac{2 \\log(1 / \\delta)}{T}} \\right) \n",
    "&\\le \\delta \\\\\n",
    "\\mathbb{P}\\left(|\\hat{\\mu} - \\mu| \\le \\sqrt{\\frac{2 \\log(1 / \\delta)}{T}} \\right) \n",
    "&\\ge 1 - 2 \\delta \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2bdee",
   "metadata": {},
   "source": [
    "# Regret decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d57e8",
   "metadata": {},
   "source": [
    "By definition, the regret after round $n$ is $R_n$ is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4347b26",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "R_n = n \\mu^* - \\mathbb{E}\\left[ \\sum_{t=1}^n X_t \\right ]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec3c53a",
   "metadata": {},
   "source": [
    "Note,\n",
    "\n",
    "* $n$ is the number of rounds of all actions so far. Note, in contrast, we use $T$ for the number of rounds corresponding to a particular arm $a$, i.e. $n = \\sum_a T_a$.\n",
    "* $\\mu^*$ is the mean reward of the best action.\n",
    "* $X_t$ is the realized reward at round $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f270de9",
   "metadata": {},
   "source": [
    "It can be rearanged to obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724e3d4",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "R_n \n",
    "&= n \\mu^* - \\mathbb{E}\\left[ \\sum_{t=1}^n X_t \\right ] \\\\\n",
    "&= n \\mu^* - \\mathbb{E}\\left[ \\sum_{t=1}^n \\sum_{a \\in \\mathcal{A}} \\mathbb{1}(A_t = a) X_{a, t} \\right ] \\\\\n",
    "&= n \\mu^* - \\mathbb{E}\\left[ \\sum_{t=1}^n \\sum_{a \\in \\mathcal{A}} \\mathbb{1}(A_t = a) \\mu_a \\right ] \\\\\n",
    "&= \\mathbb{E} \\left[ \\sum_{t=1}^n \\sum_{a \\in \\mathcal{A}} \\mathbb{1}(A_t = a)  ( \\mu^* - \\mu_a ) \\right ] \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A} } ( \\mu^* - \\mu_a) \\mathbb{E} \\left[  \\sum_{t=1}^n  \\mathbb{1}(A_t = a)  \\right ] \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A} } \\Delta_a \\mathbb{E}[T_a(n)]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf3832",
   "metadata": {},
   "source": [
    "Note,\n",
    "\n",
    "* 1st equality is the definition of regret after $n$ rounds.\n",
    "* $\\mu^*$ is the mean reward of the best action.\n",
    "* $X_t$ is the reward obtained at round $t$.\n",
    "* $\\mu_a$ is the mean reward of action $a$.\n",
    "* $A_t$ is the action played at round $t$.\n",
    "* $\\mathcal{A}$ is the set of all actions.\n",
    "* In the 4th equation, $\\mu^* - \\mu_a$ is a constant given action $a$.\n",
    "* In the 6th equation:\n",
    "  * $\\Delta_{a} = \\mu^* - \\mu_a \\ge 0$ is called the suboptimality gap for action $a$, i.e. the difference between the mean reward of the best action and that of action $a$.\n",
    "  * $T_a(n) = \\sum_{t=1}^n  \\mathbb{1}(A_t = a)$ is the number of times out of $n$ rounds action $a$ is played."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becbd804",
   "metadata": {},
   "source": [
    "# Regret analysis for upper confidence bound (UCB) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6309a0",
   "metadata": {},
   "source": [
    "In UCB, we also select the action that has the largest upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61355f5d",
   "metadata": {},
   "source": [
    "We define good event $E$ as when the inequality from concentration analysis holds $|\\hat{\\mu} - \\mu| \\le \\sqrt{\\frac{2 \\log(1 / \\delta)}{T}}$ for all actions in all rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d9938",
   "metadata": {},
   "source": [
    "Let's denote \n",
    "\n",
    "* $\\hat{\\mu}_a(t)$ as the mean estimation for action $a$ after $t$ rounds.\n",
    "* $T_a(t)$ as the number of times action $a$ is played after $t$ rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f80b6b",
   "metadata": {},
   "source": [
    "Under the good event, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a7127",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mu_a + \\sqrt{\\frac{2 \\log(1/\\delta)}{T_a(t)}} \n",
    "&\\ge \\hat{\\mu}_a(t) \\\\\n",
    "\\mu_a + 2 \\sqrt{\\frac{2 \\log(1/\\delta)}{T_a(t)}} \n",
    "&\\ge \\hat{\\mu}_a(t) + \\sqrt{\\frac{2 \\log(1/\\delta)}{T_a(t)}} \\\\\n",
    "&\\ge \\hat{\\mu}_{a^*}(t) + \\sqrt{\\frac{2 \\log(1/\\delta)}{T_{a^*}(t)}} \\\\\n",
    "&\\ge \\mu_{a^*} \\\\\n",
    "\\mu_a^* - \\mu_a = \\Delta_a\n",
    "&\\le 2 \\sqrt{\\frac{2 \\log(1/\\delta)}{T_a(t)}} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b58c39",
   "metadata": {},
   "source": [
    "Equality 2-4 are so-call the UCB (upper confidence bound) trick.\n",
    "\n",
    "* the 2nd inequality just adds $\\sqrt{\\frac{2 \\log(1/\\delta)}{T_a(t)}}$ to both sides of the 1st inequality\n",
    "* the 3rd inequality holds because we're using the UCB to select the best action, so the UCB of action $a$ must be better than or equal to that of the best action after round $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0cf0f4",
   "metadata": {},
   "source": [
    "So we have a bound for the suboptimality gap $\\Delta_a$ for $t - 1$ rounds. Rearranging 5th inequality, we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3c8311",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "T_a(t) &\\le \\frac{8 \\log(1 / \\delta)}{\\Delta_a^2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c200c",
   "metadata": {},
   "source": [
    "To use the above inequality in the decomposed regret in order to bound the regret, we need to calculate $\\mathbb{E}[T_a(n)]$ first. By definition,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176abc5",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{E}[T_a(n)] \n",
    "&= \\mathbb{E}[T_a(n) | E] \\mathbb{P}(E) + \\mathbb{E}[T_a(n) | E^c] \\mathbb{P}[E^c]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a769f7e9",
   "metadata": {},
   "source": [
    "See appendix for a quick proof of $\\mathbb{E}[Y] = \\sum_i \\mathbb{E}[Y|X_i] \\mathbb{P}(x_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128de06",
   "metadata": {},
   "source": [
    "We have already had \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[T_a(n) | E] \n",
    "&\\le \\frac{8 \\log(1 / \\delta)}{\\Delta_a^2} \\\\\n",
    "\\mathbb{P}(E)\n",
    "&\\le 1 \\\\\n",
    "\\mathbb{E}[T_a(n)|E^c] \n",
    "&\\le n\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72e407",
   "metadata": {},
   "source": [
    "The 3rd inequality is because we can play any arm at most $n$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019655e6",
   "metadata": {},
   "source": [
    "So we only need to bound $\\mathbb{P}(E^c)$ now. The concentration analysis shows that for any round $t$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b16128b",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{P}\\left( \\hat{\\mu}_{a, t} \\ge \\mu_a + \\sqrt{\\frac{2 \\log(1 / \\delta)}{t}} \\right) \\le \\delta\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fadc366",
   "metadata": {},
   "source": [
    "Then for all $n$ rounds, using union bound and one-tape assupmtion (<span style=\"color:red\">still needs to think more about this assumption</span>),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd5fc9",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{P}\\left( \\forall t \\le n; \\hat{\\mu}_{a, t} \\ge \\mu_a + \\sqrt{\\frac{2 \\log(1 / \\delta)}{t}} \\right) \\le n\\delta\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58e4f7",
   "metadata": {},
   "source": [
    "Similary, we can get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251bae6",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{P}\\left( \\forall t \\le n; \\hat{\\mu}_{a, t} \\le \\mu_a - \\sqrt{\\frac{2 \\log(1 / \\delta)}{t}} \\right) \\le n\\delta\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1287f",
   "metadata": {},
   "source": [
    "so, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82d5a5",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{P}(E^c) \\le 2n\\delta\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6745182",
   "metadata": {},
   "source": [
    "Therefore, we can bound $\\mathbb{E}[T_a(n)]$ as "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6331f748",
   "metadata": {},
   "source": [
    " \\begin{align*}\n",
    "\\mathbb{E}[T_a(n)] \n",
    "&\\le \\left( \\frac{8 \\log(1 / \\delta)}{\\Delta_a^2}  \\right ) \\cdot 1 + n \\cdot 2 n \\delta \\\\\n",
    "&= \\frac{8 \\log(1 / \\delta)}{\\Delta_a^2} + 2n^2\\delta  \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef5aef",
   "metadata": {},
   "source": [
    "Plugging it into the decomposed regret, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427d004",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "R_n \n",
    "&= \\sum_{a \\in \\mathcal{A\n",
    "}} \\Delta_a \\mathbb{E}\\left[ T_a(n) \\right ] \\\\\n",
    "&\\le \\sum_{a \\in \\mathcal{A}} \\Delta_a \\left( 2n^2 \\delta + \\frac{8 \\log(1 / \\delta)}{\\Delta_a^2} \\right )\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80df6f3",
   "metadata": {},
   "source": [
    "We can choose $\\delta$ to minimize the RHS, choosing $\\delta = \\frac{1}{n^2}$ (Not the actual minimizer), "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39637d52",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "R_n \n",
    "&\\le \\sum_{a \\in \\mathcal{A}} \\Delta_a \\left( 2 + \\frac{16 \\log(n)}{\\Delta_a^2} \\right ) \\\\\n",
    "&\\le \\sum_{a \\in \\mathcal{A}} 2 \\Delta_a + \\frac{16 \\log(n)}{\\Delta_a}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263acbdc",
   "metadata": {},
   "source": [
    "Note, **RHS can be quite big if $\\Delta_a$ is very small for long horizon application**. Hence, we can consider $R_n$ for $\\Delta_a$ that is below or above some cutoff $\\Delta$, separately,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac7868",
   "metadata": {},
   "source": [
    " \\begin{align*}\n",
    "R_n \n",
    "&= \\sum_{a \\in \\mathcal{A}: \\Delta_a \\le \\Delta} \\Delta_a \\mathbb{E}[T_a(n)] + \\sum_{a \\in \\mathcal{A}: \\Delta_a > \\Delta} \\Delta_a \\mathbb{E}[T_a(n)] \\\\\n",
    "&\\le n\\Delta + \\sum_{a \\in \\mathcal{A}: \\Delta_a > \\Delta} 2 \\Delta_a + \\frac{16 \\log(n)}{\\Delta_a} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa8948",
   "metadata": {},
   "source": [
    "Note,\n",
    "\n",
    "* $\\sum_{a \\in \\mathcal{A}: \\Delta_a \\le \\Delta} \\Delta_a \\mathbb{E}[T_a(n)]  \\le n\\Delta$ because we can play the arms at most $n$ times and they have all have $\\Delta_a \\le \\Delta$ by definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337647ac",
   "metadata": {},
   "source": [
    "We can minimize RHS by choosing $\\Delta = \\sqrt\\frac{{k \\log n}}{n}$ (<span style=\"color:red\">still needs to confirm how it's calculated</span> the derivation below isn't correct, see `Prove-regret-bounds-UCB-with-stochastic-Bernoulli-bandits.ipynb` for a more proper derivation), where $k$ is the number of arms,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ecb1b",
   "metadata": {},
   "source": [
    " \\begin{align*}\n",
    "n\\Delta\n",
    "&= \\sqrt{n k \\log n} \\\\\n",
    "\\sum_{a \\in \\mathcal{A}: \\Delta_a > \\Delta} 2 \\Delta_a\n",
    "&\\le 2k \\sqrt{\\frac{k \\log n}{n}} \\le 2 \\sqrt{k \\log n} \\le 2 \\sqrt{n \\log n} \\\\\n",
    "\\sum_{a \\in \\mathcal{A}: \\Delta_a > \\Delta} \\frac{16 \\log(n)}{\\Delta_a} \n",
    "&\\le 16 \\log n \\sqrt{\\frac{n}{k \\log n}} =16 \\sqrt{\\frac{n \\log n}{k}} \\le 16 \\sqrt{n \\log n}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ff54f",
   "metadata": {},
   "source": [
    "Note, we assume $n \\ge k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847fcdaa",
   "metadata": {},
   "source": [
    "Therefore,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578c0a5",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "R_n &= O(\\sqrt{n k \\log n})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abaec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68f35423",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07028e",
   "metadata": {},
   "source": [
    "### Concentration analysis for Gaussian r.v. with mean $\\mu$ and variance $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46526ecd",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathbb{P}(\\hat{\\mu} \\ge \\mu + \\epsilon) \n",
    "&= \\mathbb{P} \\left(\\frac{1}{T} \\sum_{t=1}^T X_t \\ge \\mu + \\epsilon \\right) \\\\\n",
    "&= \\mathbb{P} \\left(\\sum_{t=1}^T (X_t - \\mu) \\ge T\\epsilon \\right) \\\\\n",
    "&= \\mathbb{P} \\left(\\lambda \\sum_{t=1}^T (X_t - \\mu) \\ge \\lambda T\\epsilon \\right) \\\\\n",
    "&= \\mathbb{P} \\left(\\exp \\left( \\lambda \\sum_{t=1}^T (X_t - \\mu) \\right ) \\ge \\exp \\left(\\lambda T\\epsilon \\right ) \\right )\\\\\n",
    "&\\le \\frac{\\mathbb{E} \\left[\\exp \\left( \\lambda \\sum_{t=1}^T (X_t - \\mu) \\right ) \\right]}{\\exp \\left( \\lambda T\\epsilon \\right )} \\\\\n",
    "&= \\frac{ \\mathbb{E} \\left[\\prod_{t=1}^T \\exp \\left( \\lambda (X_t - \\mu)\\right ) \\right]}{\\exp \\left( \\lambda T\\epsilon \\right )} \\\\\n",
    "&= \\frac{ \\prod_{t=1}^T \\mathbb{E} \\left[ \\exp \\left( \\lambda (X_t - \\mu)\\right ) \\right]}{\\exp \\left( \\lambda T\\epsilon\\right )} \\\\\n",
    "&= \\frac{ \\prod_{t=1}^T \\exp \\left( \\frac{\\lambda ^2}{2} \\right )}{ \\exp \\left( \\lambda T\\epsilon\\right )} \\\\\n",
    "&= \\exp \\left( \\frac{\\lambda ^2 T}{2} - \\lambda T\\epsilon \\right ) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1615f",
   "metadata": {},
   "source": [
    "Note,\n",
    "\n",
    "* $\\lambda$ is a positive variable we can choose freely.\n",
    "* In 4th equality, we take the exponential so that the quantity becomes positive and Markov's inequality can be applied.\n",
    "* the 5th inequality is a direct application of Markov's inequality: $\\mathbb{E}[Z \\ge t] = \\frac{\\mathbb{E}[Z]}{t}$ for non-negative random variable, $Z$\n",
    "* the 6th equality holds because $\\mathbb{E}\\left[\\prod X_i \\right] = \\prod \\mathbb{E}\\left[X_i\\right]$ for independent random variables. A quick proof for two r.v.s with $f(x_1)$ and $f(x_2)$ being the probability density functions,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[X_1 \\cdot X_2] \n",
    "&= \\iint_{x_1, x_2} f(x_1 \\cdot x_2) dx_1 dx_2 \\\\\n",
    "&= \\iint_{x_1, x_2} f(x_1) f(x_2) dx_1 dx_2 \\\\\n",
    "&= \\int_{x_1} f(x_1) dx_1 \\int_{x_2} f(x_2) dx_2 \\\\\n",
    "&= \\mathbb{E}[X_1] \\mathbb{E}[X_2]\n",
    "\\end{align*}\n",
    "\n",
    "* the 7th equality is an application of the MGF for Standard Gaussian r.v. with $\\mu = 0$ and $\\sigma^2=1$, $M_x(\\lambda) = \\mathbb{E}[e^{\\lambda X}] = e^{\\lambda^2 / 2}$. A quick proof:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[e^{\\lambda X}]\n",
    "&= \\int \\exp(\\lambda x) \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(- \\frac{1}{2}x^2 \\right ) dx \\\\\n",
    "&= \\frac{1}{\\sqrt{2 \\pi}} \\int \\exp \\left(\\lambda x - \\frac{1}{2}x^2 \\right ) dx \\\\\n",
    "&= \\frac{1}{\\sqrt{2 \\pi}} \\int \\exp \\left(- \\frac{1}{2} \\left(x^2 - 2\\lambda x  \\right ) \\right ) dx \\\\\n",
    "&= \\frac{1}{\\sqrt{2 \\pi}} \\int \\exp \\left(- \\frac{1}{2} \\left( x - \\lambda)^2 - \\lambda^2 \\right ) \\right ) dx \\\\\n",
    "&= \\exp \\left(\\frac{\\lambda ^2}{2} \\right ) \\frac{1}{\\sqrt{2 \\pi}}  \\int \\exp \\left(- \\frac{1}{2} (x - \\lambda)^2 \\right ) dx \\\\\n",
    "&= \\exp \\left(\\frac{\\lambda ^2}{2} \\right )\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17801f0",
   "metadata": {},
   "source": [
    "A derivation of MGF for the more general $\\mathcal{N}(\\mu, \\sigma^2)$ is available [here](https://zyxue.github.io/2021/08/29/gaussian-distributions.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6c46a",
   "metadata": {},
   "source": [
    "Given the RHS in 9th equality is just a quadratic function, and we can choose $\\lambda$ freely,\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\lambda} \\exp \\left( \\frac{\\lambda ^2 T}{2} - \\lambda T\\epsilon \\right ) = \\exp \\left( - \\frac{\\epsilon^2T}{2} \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda6836",
   "metadata": {},
   "source": [
    "So "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6eaba",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{P}(\\hat{\\mu} \\ge \\mu + \\epsilon) \\le \\exp \\left( - \\frac{\\epsilon^2 T}{2} \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc55f34",
   "metadata": {},
   "source": [
    "Note, the bound for $\\mathbb{P}(\\hat{\\mu} \\le \\mu - \\epsilon)$ can be obtained by deriving $\\mathbb{P}(- \\hat{\\mu} \\ge - \\mu + \\epsilon) = \\mathbb{P}( \\mu - \\hat{\\mu} \\ge + \\epsilon) $ using the same logic, and it turns out the lower bound has the same form, i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee9579",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{P}(\\hat{\\mu} \\le \\mu - \\epsilon) \\le \\exp \\left( - \\frac{\\epsilon^2 T}{2} \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1205643b",
   "metadata": {},
   "source": [
    "This is because $X_t - \\mu$ and $\\mu - X_t$ have the same distribution when $X_t$ is Gaussian with mean $\\mu$, which means $\\mu - \\hat{\\mu}$ and $\\hat{\\mu} - \\mu$ also have the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349725a1",
   "metadata": {},
   "source": [
    "### Prove $\\mathbb{E}[Y] = \\sum_i \\mathbb{E}[Y|X_i] \\mathbb{P}(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffdbed0",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{E}[Y] \n",
    "&= \\int_y y f(y) dy \\\\\n",
    "&= \\int_y y \\sum_i f(y|x_i) \\mathbb{P}(x_i) dy \\\\\n",
    "&= \\sum_i \\int_y p(y|x_i)dy \\mathbb{P}(x_i) \\\\\n",
    "&= \\sum_i \\mathbb{E}[Y|X_i] \\mathbb{P}(x_i)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7efdb6e",
   "metadata": {},
   "source": [
    "Note, here we have $Y$ being continuous and $X$ being discrete. $f(y)$ is the PDF of $Y$ and $\\mathbb{P}$ is the PMF of $X_i$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
